### Reference

+ SQuAD，斯坦福在自然语言处理的野心
	+ http://blog.csdn.net/jdbc/article/details/52514050

- SOTAs
  - Hybrid AoA Reader (ensemble)
  	- Joint Laboratory of HIT and iFLYTEK Research
  - r-net + 融合模型
  	- Microsoft Research Asia
  - SLQA + 融合模型
  	- Alibaba iDST NLP
- detail
  - 这个竞赛基于SQuAD问答数据集，考察两个指标：EM和F1。
  - EM是指精确匹配，也就是模型给出的答案与标准答案一模一样；
  - F1，是根据模型给出的答案和标准答案之间的重合度计算出来的，也就是结合了召回率和精确率。
  - 目前阿里、微软团队并列第一，其中EM得分微软（r-net+融合模型）更高，F1得分阿里（SLQA+融合模型）更高。但是他们在EM成绩上都击败了“人类表现”
  - 一共有107,785问题，以及配套的 536 篇文章
  	- 数据集的具体构建如下：
  		​    1. 文章是随机sample的wiki百科，一共有536篇wiki被选中。而每篇wiki，会被切成段落，最终生成了23215个自然段。之后就对这23215个自然段进行阅读理解，或者说自动问答。
  		​    2. 之后斯坦福，利用众包的方式，进行了给定文章，提问题并给答案的人工标注。他们将这两万多个段落给不同人，要求对每个段落提五个问题。
  		​    3. 让另一些人对提的这个问题用文中最短的片段给予答案，如果不会或者答案没有在文章中出现可以不给。之后经过他们的验证，人们所提的问题在问题类型分布上足够多样，并且有很多需要推理的问题，也就意味着这个集合十分有难度。如下图所示，作者列出了该数据集答案的类别分布，我们可以看到 日期，人名，地点，数字等都被囊括，且比例相当。
  		​    4. 这个数据集的评测标准有两个：
  		​        第一：F1
  		​    ​    第二：EM。
  		​        ​        EM是完全匹配的缩写，必须机器给出的和人给出的一样才算正确。哪怕有一个字母不一样，也会算错。而F1是将答案的短语切成词，和人的答案一起算recall，Precision和F1，即如果你match了一些词但不全对，仍然算分。
  		​    5. 为了这个数据集，他们还做了一个baseline，是通过提特征，用LR算法将特征组合，最终达到了40.4的em和51的f1。而现在IBM和新加坡管理大学利用深度学习模型，均突破了这个算法。可以想见，在不远的将来会有更多人对阅读理解发起挑战，自然语言的英雄也必将诞生。甚至会有算法超过人的准确度。